{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "ChatOllama\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"ChatOllama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama 모델 사용\n",
    "\n",
    "한국어 잘하는 Open 모델 \n",
    "\n",
    "**참고**\n",
    "\n",
    "각 모델의 라이센스를 반드시 확인 후 사용해주세요.\n",
    "\n",
    "- EXAONE-3.5 모델(gguf): https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-GGUF\n",
    "- gemma2-27b: https://ollama.com/library/gemma2:27b\n",
    "- EEVE-Korean-10.8B(gguf): https://huggingface.co/teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf\n",
    "- Qwen2.5-7B-Instruct-kowiki-qa-context(gguf): https://huggingface.co/teddylee777/Qwen2.5-7B-Instruct-kowiki-qa-gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "딥 러닝(Deep Learning)은 인공지능(AI)의 한 분야로, 인공 신경망(Artificial Neural Networks)을 기반으로 하는 머신 러닝 기법입니다. 주요 특징과 개념은 다음과 같습니다:\n",
      "\n",
      "1. **다층 신경망 구조**: 딥 러닝은 여러 층(layer)으로 구성된 복잡한 신경망을 사용합니다. 이러한 다층 구조를 통해 데이터의 고차원적 특징을 자동으로 학습하고 추출할 수 있습니다. 예를 들어, 입력 데이터가 이미지일 경우, 초기 층은 가장자리나 간단한 패턴을 인식하고, 더 깊은 층은 복잡한 객체나 구조를 인식하게 됩니다.\n",
      "\n",
      "2. **자동 특징 추출**: 전통적인 머신 러닝 방법에서는 특징 공학(feature engineering)이 필요했습니다. 즉, 전문가가 데이터에서 중요한 특징을 수동으로 추출해야 했습니다. 하지만 딥 러닝은 이러한 특징 추출 과정을 자동화하여, 대량의 데이터를 통해 스스로 중요한 특징을 학습합니다.\n",
      "\n",
      "3. **대규모 데이터 활용**: 딥 러닝 모델은 대량의 데이터를 통해 훈련될 때 가장 효과적입니다. 데이터의 양이 많을수록 모델은 더 정확하고 복잡한 패턴을 학습할 수 있습니다. 이는 빅 데이터 시대에 특히 유리한 특징입니다.\n",
      "\n",
      "4. **응용 분야**:\n",
      "   - **이미지 및 비디오 인식**: 객체 인식, 이미지 분류, 세그멘테이션 등\n",
      "   - **자연어 처리(NLP)**: 텍스트 분류, 번역, 챗봇, 감성 분석 등\n",
      "   - **음성 인식**: 음성 명령 인식, 자동 음성 응답 시스템 등\n",
      "   - **추천 시스템**: 온라인 쇼핑, 스트리밍 서비스의 개인화 추천 등\n",
      "\n",
      "5. **알고리즘 예시**:\n",
      "   - **컨볼루션 신경망 (CNN, Convolutional Neural Networks)**: 주로 이미지와 비디오 데이터에 사용됩니다.\n",
      "   - **순환 신경망 (RNN, Recurrent Neural Networks)**: 시계열 데이터나 순차적인 데이터(예: 텍스트, 음성) 처리에 적합합니다.\n",
      "   - **전결합 신경망 (Fully Connected Networks)**: 초기 딥 러닝 모델의 형태로, 다양한 유형의 데이터에 적용 가능합니다.\n",
      "   - **Transformer 모델**: 최근 NLP 분야에서 뛰어난 성능을 보여주는 모델로, BERT, GPT 시리즈 등이 있습니다.\n",
      "\n",
      "딥 러닝은 복잡한 문제 해결에 있어 뛰어난 성능을 보여주지만, 훈련에 필요한 컴퓨팅 자원과 데이터 양이 많다는 단점도 있습니다. 그럼에도 불구하고, 지속적인 기술 발전으로 인해 다양한 산업 분야에서 광범위하게 활용되고 있습니다."
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# Ollama 모델 지정\n",
    "llm = ChatOllama(\n",
    "    model=\"exaone\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = ChatPromptTemplate.from_template(\"{topic} 에 대하여 간략히 설명해 줘.\")\n",
    "\n",
    "# 체인 생성\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 스트림 출력\n",
    "answer = chain.stream({\"topic\": \"deep learning\"})\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def fibonacci(n):\n",
      "  if n <= 1:\n",
      "    return n\n",
      "  else:\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "# 원하는 항까지 피보나치 수열 출력\n",
      "for i in range(10):\n",
      "  print(fibonacci(i))\n",
      "```"
     ]
    }
   ],
   "source": [
    "# gemma2-27b\n",
    "llm = ChatOllama(\n",
    "    model=\"gemma2:27b\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 주제를 기반으로 짧은 농담을 요청하는 프롬프트 템플릿을 생성합니다.\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the following question in Korean.\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")\n",
    "\n",
    "# LangChain 표현식 언어 체인 구문을 사용합니다.\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 체인 실행\n",
    "answer = chain.stream({\"question\": \"python 코드로 피보나치 수열을 구현해보세요.\"})\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OllamaEmbeddings 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# 임베딩 설정\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코사인 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"안녕하세요? 반갑습니다.\"\n",
    "sentence2 = \"안녕하세요? 반갑습니다!\"\n",
    "sentence3 = \"안녕하세요? 만나서 반가워요.\"\n",
    "sentence4 = \"Hi, nice to meet you.\"\n",
    "sentence5 = \"I like to eat apples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사도 계산을 위한 임베딩을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentences = [sentence1, sentence2, sentence3, sentence4, sentence5]\n",
    "embedded_sentences = embeddings.embed_documents(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(a, b):\n",
    "    return cosine_similarity([a], [b])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사도 계산 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[유사도 0.9800] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 반갑습니다!\n",
      "[유사도 0.9401] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
      "[유사도 0.8568] 안녕하세요? 반갑습니다. \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.5516] 안녕하세요? 반갑습니다. \t <=====> \t I like to eat apples.\n",
      "[유사도 0.9203] 안녕하세요? 반갑습니다! \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
      "[유사도 0.8385] 안녕하세요? 반갑습니다! \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.5307] 안녕하세요? 반갑습니다! \t <=====> \t I like to eat apples.\n",
      "[유사도 0.9254] 안녕하세요? 만나서 반가워요. \t <=====> \t Hi, nice to meet you.\n",
      "[유사도 0.5611] 안녕하세요? 만나서 반가워요. \t <=====> \t I like to eat apples.\n",
      "[유사도 0.5955] Hi, nice to meet you. \t <=====> \t I like to eat apples.\n"
     ]
    }
   ],
   "source": [
    "# sentence1 = \"안녕하세요? 반갑습니다.\"\n",
    "# sentence2 = \"안녕하세요? 반갑습니다!\"\n",
    "# sentence3 = \"안녕하세요? 만나서 반가워요.\"\n",
    "# sentence4 = \"Hi, nice to meet you.\"\n",
    "# sentence5 = \"I like to eat apples.\"\n",
    "\n",
    "for i, sentence in enumerate(embedded_sentences):\n",
    "    for j, other_sentence in enumerate(embedded_sentences):\n",
    "        if i < j:\n",
    "            print(\n",
    "                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습에 활용한 문서**\n",
    "\n",
    "소프트웨어정책연구소(SPRi) - 2023년 12월호\n",
    "\n",
    "- 저자: 유재흥(AI정책연구실 책임연구원), 이지수(AI정책연구실 위촉연구원)\n",
    "- 링크: https://spri.kr/posts/view/23669\n",
    "- 파일명: `SPRI_AI_Brief_2023년12월호_F.pdf`\n",
    "\n",
    "_실습을 위해 다운로드 받은 파일을 `data` 폴더로 복사해 주시기 바랍니다_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "# 문서 로드\n",
    "loader = PDFPlumberLoader(\"data/SPRI_AI_Brief_2023년12월호_F.pdf\")\n",
    "\n",
    "# 문서 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "split_docs = loader.load_and_split(text_splitter)\n",
    "\n",
    "# 임베딩 설정\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"bge-m3\",\n",
    ")\n",
    "\n",
    "# 벡터스토어 생성\n",
    "vectorstore = FAISS.from_documents(documents=split_docs, embedding=embeddings)\n",
    "\n",
    "# 검색기 생성\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# 프롬프트 로드\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Answer in Korean.\n",
    "\n",
    "Please follow these instructions:\n",
    "\n",
    "1. Analyze the content of the source documents: \n",
    "2. The name of each source document is at the start of the document, with the <document> tag.\n",
    "\n",
    "-----\n",
    "\n",
    "Output format should be like this:\n",
    "\n",
    "(Your comprehensive answer to the question)\n",
    "\n",
    "**Source**\n",
    "- [1] Document source with page number\n",
    "- [2] Document source with page number\n",
    "(...)\n",
    "\n",
    "-----\n",
    "\n",
    "### Here is the context that you can use to answer the question:\n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "### Here is user's question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your answer to the question:\n",
    "\n",
    "### Answer:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"<document><content>{doc.page_content}</content><page>{doc.metadata['page']}</page><source>{doc.metadata['source']}</source></document>\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "\n",
    "# Ollama 모델 지정\n",
    "llm = ChatOllama(\n",
    "    model=\"exaone\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# 체인 생성\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삼성전자가 개발한 생성형 AI의 이름은 **삼성 가우스(Samsung Gauss)**입니다.\n",
      "\n",
      "**Source**\n",
      "- [1] data/SPRI_AI_Brief_2023년12월호_F.pdf (페이지 12)\n",
      "- [2] data/SPRI_AI_Brief_2023년12월호_F.pdf (페이지 12)\n",
      "- [3] data/SPRI_AI_Brief_2023년12월호_F.pdf (페이지 12)"
     ]
    }
   ],
   "source": [
    "question = \"삼성전자가 개발한 생성형 AI 의 이름은?\"\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.stream(question)\n",
    "# 스트림 출력\n",
    "stream_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
